# -*- coding: utf-8 -*-
"""ipattn.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1m30_6pz-5y_4u_2g2qvR7PCONL7SZIZ8
"""

import torch
from diffusers import StableDiffusionPipeline, AutoencoderKL
from transformers import CLIPImageProcessor, CLIPVisionModelWithProjection
import matplotlib.pyplot as plt
from embedding_helpers import EmbeddingUtil
from pipelines import CompatibleLatentConsistencyModelPipeline
from PIL import Image
import gc
from controlnet_aux import HEDdetector, MidasDetector, MLSDdetector, OpenposeDetector, PidiNetDetector, NormalBaeDetector, LineartDetector, LineartAnimeDetector, CannyDetector, ContentShuffleDetector, ZoeDetector, MediapipeFaceDetector, SamDetector, LeresDetector, DWposeDetector
from custom_sam_detector import CustomSamDetector

import os
import warnings
from typing import Union

import cv2
import numpy as np
import torch
from huggingface_hub import hf_hub_download
from PIL import Image

# Load human segmentation preprocessor
sam =  SamDetector.from_pretrained("ybelkada/segment-anything", subfolder="checkpoints")

from diffusers.utils.loading_utils import load_image
from huggingface_hub import create_repo,HfApi


'''gen=torch.Generator()
gen.manual_seed(123)
gen_image=pipe("cat",height=dim,width=dim,num_inference_steps=4,ip_adapter_image=ip_adapter_image,generator=gen)

gen_image.images[0]

[(n[0],n[1].__class__) for n in pipe.unet.named_modules() if n[0].endswith("processor")]

pipe.unet.mid_block.attentions[0].transformer_blocks[0].attn2.processor.scale'''

from diffusers.models.attention_processor import  IPAdapterAttnProcessor2_0,Attention
from diffusers.utils import deprecate, is_torch_xla_available, logging
from typing import Optional,List
from diffusers.image_processor import IPAdapterMaskProcessor
# Copyright 2025 The HuggingFace Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
import inspect
from adapter_helpers import replace_ip_attn,get_modules_of_types
import math
from typing import Callable, List, Optional, Tuple, Union

import torch
import torch.nn.functional as F
from main_pers import concat_images_horizontally,concat_images_vertically
from PIL import Image, ImageDraw, ImageFont
import os
import sys

os.makedirs("ip_images",exist_ok=True)

sam =  SamDetector.from_pretrained("ybelkada/segment-anything", subfolder="checkpoints")

big_global_dict={}
big_global_ip_dict={}

def add_margin(pil_img, top, right, bottom, left, color):
    width, height = pil_img.size
    new_width = width + right + left
    new_height = height + top + bottom
    result = Image.new(pil_img.mode, (new_width, new_height), color)
    result.paste(pil_img, (left, top))
    return result

def add_padding_with_text(img: Image.Image, text: str, pad_width: int = 100, font_path=None, font_size=24):
    """
    Add white padding to the left of an image and draw text in that space.

    Args:
        img (PIL.Image): Input image.
        text (str): Text to write in the padding area.
        pad_width (int): Width of the white padding to add on the left.
        font_path (str): Optional path to a .ttf font file.
        font_size (int): Font size for the text.
    """
    w, h = img.size
    
    # Create new white canvas with extra width
    new_img = Image.new("RGB", (w + pad_width, h), "white")

    # Paste original image on the right
    new_img.paste(img, (pad_width, 0))

    # Prepare to draw
    draw = ImageDraw.Draw(new_img)
    
    # Load font (default to PIL built-in if no path given)
    if font_path:
        font = ImageFont.truetype(font_path, font_size)
    else:
        font = ImageFont.load_default()

    

    # Draw text in black
    draw.text((h//8, h//2), text, fill="black", font_size=font_size)

    return new_img

class MonkeyIPAttnProcessor(torch.nn.Module):
    def __init__(self,processor:IPAdapterAttnProcessor2_0,dict_name:str, *args, **kwargs) -> None:
        super().__init__(*args, **kwargs)
        self.processor=processor
        self.scale=processor.scale
        self.dict_name=dict_name
        self.kv=[]
        self.kv_ip=[]

    def reset(self):
        self.kv.clear()
        self.kv_ip.clear()
        gc.collect()
        torch.cuda.empty_cache()

    def __call__(self,
        attn: Attention,
        hidden_states: torch.Tensor,
        encoder_hidden_states: Optional[torch.Tensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
        temb: Optional[torch.Tensor] = None,
        scale: float = 1.0,
        ip_adapter_masks: Optional[torch.Tensor] = None,
    ):
        #print("h")
        residual = hidden_states

        # separate ip_hidden_states from encoder_hidden_states
        if encoder_hidden_states is not None:
            if isinstance(encoder_hidden_states, tuple):
                encoder_hidden_states, ip_hidden_states = encoder_hidden_states
            else:
                deprecation_message = (
                    "You have passed a tensor as `encoder_hidden_states`. This is deprecated and will be removed in a future release."
                    " Please make sure to update your script to pass `encoder_hidden_states` as a tuple to suppress this warning."
                )
                deprecate("encoder_hidden_states not a tuple", "1.0.0", deprecation_message, standard_warn=False)
                end_pos = encoder_hidden_states.shape[1] - self.processor.num_tokens[0]
                encoder_hidden_states, ip_hidden_states = (
                    encoder_hidden_states[:, :end_pos, :],
                    [encoder_hidden_states[:, end_pos:, :]],
                )

        if attn.spatial_norm is not None:
            hidden_states = attn.spatial_norm(hidden_states, temb)

        input_ndim = hidden_states.ndim

        #print("hidden states shape",hidden_states.size())

        if input_ndim == 4:
            batch_size, channel, height, width = hidden_states.shape
            hidden_states = hidden_states.view(batch_size, channel, height * width).transpose(1, 2)
        #print("\t hidden states shape",hidden_states.size())
        batch_size, sequence_length, _ = (
            hidden_states.shape if encoder_hidden_states is None else encoder_hidden_states.shape
        )

        if attention_mask is not None:
            attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)
            # scaled_dot_product_attention expects attention_mask shape to be
            # (batch, heads, source_length, target_length)
            attention_mask = attention_mask.view(batch_size, attn.heads, -1, attention_mask.shape[-1])

        if attn.group_norm is not None:
            hidden_states = attn.group_norm(hidden_states.transpose(1, 2)).transpose(1, 2)

        query = attn.to_q(hidden_states)
        #print("\t query size",query.size())

        if encoder_hidden_states is None:
            encoder_hidden_states = hidden_states
        elif attn.norm_cross:
            encoder_hidden_states = attn.norm_encoder_hidden_states(encoder_hidden_states)

        key = attn.to_k(encoder_hidden_states)
        #print("\t k size",key.size())
        value = attn.to_v(encoder_hidden_states)

        inner_dim = key.shape[-1]
        head_dim = inner_dim // attn.heads

        query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)
        #print("\t query size",query.size())

        key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)
        #print("\t k size",key.size())
        value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)

        # the output of sdp = (batch, num_heads, seq_len, head_dim)
        # TODO: add support for attn.scale when we move to Torch 2.1
        hidden_states = F.scaled_dot_product_attention(
            query, key, value, attn_mask=attention_mask, dropout_p=0.0, is_causal=False
        )

        #print("\t hidden states shape after scaled dot product",hidden_states.size())
        attn_weight = query @ key.transpose(-2, -1)
        attn_weight = torch.softmax(attn_weight, dim=-1)

        #print("\t attn_weight shape",attn_weight.size())

        self.kv.append(attn_weight)

        hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)
        #print("\t hidden states shape after transpose",hidden_states.size())
        hidden_states = hidden_states.to(query.dtype)

        if ip_adapter_masks is not None:
            if not isinstance(ip_adapter_masks, List):
                # for backward compatibility, we accept `ip_adapter_mask` as a tensor of shape [num_ip_adapter, 1, height, width]
                ip_adapter_masks = list(ip_adapter_masks.unsqueeze(1))
            if not (len(ip_adapter_masks) == len(self.processor.scale) == len(ip_hidden_states)):
                raise ValueError(
                    f"Length of ip_adapter_masks array ({len(ip_adapter_masks)}) must match "
                    f"length of self.processor.scale array ({len(self.processor.scale)}) and number of ip_hidden_states "
                    f"({len(ip_hidden_states)})"
                )
            else:
                for index, (mask, scale, ip_state) in enumerate(zip(ip_adapter_masks, self.processor.scale, ip_hidden_states)):
                    if mask is None:
                        continue
                    if not isinstance(mask, torch.Tensor) or mask.ndim != 4:
                        raise ValueError(
                            "Each element of the ip_adapter_masks array should be a tensor with shape "
                            "[1, num_images_for_ip_adapter, height, width]."
                            " Please use `IPAdapterMaskProcessor` to preprocess your mask"
                        )
                    if mask.shape[1] != ip_state.shape[1]:
                        raise ValueError(
                            f"Number of masks ({mask.shape[1]}) does not match "
                            f"number of ip images ({ip_state.shape[1]}) at index {index}"
                        )
                    if isinstance(scale, list) and not len(scale) == mask.shape[1]:
                        raise ValueError(
                            f"Number of masks ({mask.shape[1]}) does not match "
                            f"number of scales ({len(scale)}) at index {index}"
                        )
        else:
            ip_adapter_masks = [None] * len(self.processor.scale)

        # for ip-adapter
        for current_ip_hidden_states, scale, to_k_ip, to_v_ip, mask in zip(
            ip_hidden_states, self.processor.scale, self.processor.to_k_ip, self.processor.to_v_ip, ip_adapter_masks
        ):
            skip = False
            if isinstance(scale, list):
                if all(s == 0 for s in scale):
                    skip = True
            elif scale == 0:
                skip = True
            if not skip:
                if mask is not None:
                    if not isinstance(scale, list):
                        scale = [scale] * mask.shape[1]

                    current_num_images = mask.shape[1]
                    for i in range(current_num_images):
                        ip_key = to_k_ip(current_ip_hidden_states[:, i, :, :])
                        ip_value = to_v_ip(current_ip_hidden_states[:, i, :, :])

                        ip_key = ip_key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)
                        ip_value = ip_value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)

                        # the output of sdp = (batch, num_heads, seq_len, head_dim)
                        # TODO: add support for attn.scale when we move to Torch 2.1
                        _current_ip_hidden_states = F.scaled_dot_product_attention(
                            query, ip_key, ip_value, attn_mask=None, dropout_p=0.0, is_causal=False
                        )

                        _current_ip_hidden_states = _current_ip_hidden_states.transpose(1, 2).reshape(
                            batch_size, -1, attn.heads * head_dim
                        )
                        _current_ip_hidden_states = _current_ip_hidden_states.to(query.dtype)

                        mask_downsample = IPAdapterMaskProcessor.downsample(
                            mask[:, i, :, :],
                            batch_size,
                            _current_ip_hidden_states.shape[1],
                            _current_ip_hidden_states.shape[2],
                        )

                        mask_downsample = mask_downsample.to(dtype=query.dtype, device=query.device)
                        hidden_states = hidden_states + scale[i] * (_current_ip_hidden_states * mask_downsample)
                else:
                    ip_key = to_k_ip(current_ip_hidden_states)
                    #print("\t ip key size",ip_key.size())
                    ip_value = to_v_ip(current_ip_hidden_states)

                    ip_key = ip_key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)
                    #print("\t ip key after view size",ip_key.size())
                    ip_value = ip_value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)

                    # the output of sdp = (batch, num_heads, seq_len, head_dim)
                    # TODO: add support for attn.scale when we move to Torch 2.1
                    current_ip_hidden_states = F.scaled_dot_product_attention(
                        query, ip_key, ip_value, attn_mask=None, dropout_p=0.0, is_causal=False
                    )

                    attn_weight = query @ ip_key.transpose(-2, -1)
                    attn_weight = torch.softmax(attn_weight, dim=-1)

                    self.kv_ip.append(attn_weight)

                    #print("\t attn_weight shape",attn_weight.size())
                    if self.dict_name not in big_global_ip_dict:
                        big_global_ip_dict[self.dict_name]=[]
                    big_global_ip_dict[self.dict_name].append(attn_weight)

                    current_ip_hidden_states = current_ip_hidden_states.transpose(1, 2).reshape(
                        batch_size, -1, attn.heads * head_dim
                    )
                    current_ip_hidden_states = current_ip_hidden_states.to(query.dtype)

                    hidden_states = hidden_states + scale * current_ip_hidden_states

        # linear proj
        hidden_states = attn.to_out[0](hidden_states)
        # dropout
        hidden_states = attn.to_out[1](hidden_states)

        if input_ndim == 4:
            hidden_states = hidden_states.transpose(-1, -2).reshape(batch_size, channel, height, width)

        if attn.residual_connection:
            hidden_states = hidden_states + residual

        hidden_states = hidden_states / attn.rescale_output_factor

        return hidden_states
    


def get_modules_of_types(model, target_classes):
        return [(name, module) for name, module in model.named_modules()
                if isinstance(module, target_classes)]

def reset_monkey(pipe):
    attn_list=get_modules_of_types(pipe.unet,MonkeyIPAttnProcessor)
    for name,module in attn_list:
        module.reset()

if __name__ =="__main__":
    use_embedding=False
    clargs=sys.argv
    print("clargs",clargs)
    if len(clargs) > 1:
        embedding_type=clargs[1]
        if embedding_type in ["clip","ssl","dino","siglip2"]:
            use_embedding=True


    count=0
    count_ip=0
    ip_adapter_image=load_image("https://assets-us-01.kc-usercontent.com/5cb25086-82d2-4c89-94f0-8450813a0fd3/0c3fcefb-bc28-4af6-985e-0c3b499ae832/Elon_Musk_Royal_Society.jpg")

    pipe = StableDiffusionPipeline.from_pretrained(
        "SimianLuo/LCM_Dreamshaper_v7",
        torch_dtype=torch.float16,
    ).to("cuda")

    # Load IP-Adapter
    pipe.load_ip_adapter("h94/IP-Adapter", subfolder="models", weight_name="ip-adapter_sd15.bin")

    if use_embedding:
        pipe = CompatibleLatentConsistencyModelPipeline.from_pretrained(
            "SimianLuo/LCM_Dreamshaper_v7",
            torch_dtype=torch.float16,
        ).to("cuda")

        # Load IP-Adapter
        pipe.load_ip_adapter("h94/IP-Adapter", subfolder="models", weight_name="ip-adapter_sd15.bin")
        facet=""
        dino_pooling_stride=4
        embedding_util=EmbeddingUtil(pipe.unet.device,torch.float16,embedding_type,facet,dino_pooling_stride)
        lr=0.001
        suffix="_identity"
        n =1000
        pipeline_name="lcm"
        reward_switch_epoch =-1
        hf_path=f"jlbaker361/denoise_epsilon_{embedding_type}_1.0_{lr}_{n}{suffix}_{pipeline_name}_{reward_switch_epoch}"

        embedding_dim={
            "clip":768,
            "siglip2":768,
            "ssl":1536,
            "dino":3456
        }[embedding_type]

        num_image_text_embeds=4
        intermediate_embedding_dim=1024

        cross_attention_dim=embedding_dim//num_image_text_embeds
        use_projection=True
        identity_adapter=True
        deep_to_ip_layers=False
        WEIGHTS_NAME="unet_model.bin"
        api=HfApi()
        replace_ip_attn(pipe.unet,
                    embedding_dim,
                    intermediate_embedding_dim,
                    cross_attention_dim,
                    num_image_text_embeds,
                    use_projection,identity_adapter,deep_to_ip_layers)
        pretrained_weights_path=api.hf_hub_download(hf_path,WEIGHTS_NAME,force_download=True)
        pipe.unet.load_state_dict(torch.load(pretrained_weights_path,weights_only=True),strict=False)
        
        print("loaded from  ",pretrained_weights_path)


    pipe.set_ip_adapter_scale(0.5)

    

    attn_list=get_modules_of_types(pipe.unet,Attention)
    for [name,_] in attn_list:
        print(name)

    for name,module in attn_list:
        if getattr(module,"processor",None)!=None and type(getattr(module,"processor",None))==IPAdapterAttnProcessor2_0:
            setattr(module,"processor",MonkeyIPAttnProcessor(module.processor,name))
    dim=512

    setattr(pipe,"safety_checker",None)

    n_tokens=6
    n_tokens_ip=4

    threshold=0.5

    gen=torch.Generator()
    gen.manual_seed(123)
    num_inference_steps=4
    for n,ip_adapter_image in enumerate([
        load_image("https://assets-us-01.kc-usercontent.com/5cb25086-82d2-4c89-94f0-8450813a0fd3/0c3fcefb-bc28-4af6-985e-0c3b499ae832/Elon_Musk_Royal_Society.jpg"),
        load_image("https://hips.hearstapps.com/hmg-prod/images/dog-puppy-on-garden-royalty-free-image-1586966191.jpg"),
        load_image("https://images.ctfassets.net/qx5k8y1u9drj/d0vHplNVGw8cRbFsHlMHY/fda93fe8b54c67d4dae79aca19f8beb2/VIDEO_CAROUSEL_IMAGE_-_SEE_FIGURE_IN_ACTION.jpg"),
        load_image("https://cmsassets.rgpub.io/sanity/images/dsfx7636/game_data_live/4238fe90dd74b08a6e8172c31e3b1ae609afb3cd-496x560.jpg"),
        load_image("https://draftsim.com/wp-content/uploads/2020/06/Oath-of-Teferi-MTG-card-art-by-Wesley-Burt-1024x752.jpg"),
        load_image("lizard.jpg"),
        load_image("rick.jpg"),
        load_image("elf.jpg"),
        load_image("ghibli.jpg")
    ]):
        for m,prompt in enumerate(["eating ice cream","in paris","in the style of cubism","on a walk"]):
            #reset_monkey(pipe)

            '''pipe = StableDiffusionPipeline.from_pretrained(
                "SimianLuo/LCM_Dreamshaper_v7",
                torch_dtype=torch.float16,
            ).to("cuda")

            # Load IP-Adapter
            pipe.load_ip_adapter("h94/IP-Adapter", subfolder="models", weight_name="ip-adapter_sd15.bin")
            pipe.set_ip_adapter_scale(0.5)

            

            attn_list=get_modules_of_types(pipe.unet,Attention)

            for name,module in attn_list:
                if getattr(module,"processor",None)!=None and type(getattr(module,"processor",None))==IPAdapterAttnProcessor2_0:
                    setattr(module,"processor",MonkeyIPAttnProcessor(module.processor,name))
            dim=512

            setattr(pipe,"safety_checker",None)'''

            reset_monkey(pipe)
            if use_embedding:
                image_embeds=embedding_util.embed_img_tensor(pipe.image_processor.preprocess(image=ip_adapter_image)).unsqueeze(0).unsqueeze(0)
                gen_image=pipe(prompt,height=dim,width=dim,num_inference_steps=num_inference_steps,ip_adapter_image_embeds=[image_embeds],generator=gen).images[0]
            else:
                gen_image=pipe(prompt,height=dim,width=dim,num_inference_steps=num_inference_steps,ip_adapter_image=ip_adapter_image,generator=gen).images[0]
            
            '''monkey_attn_list=get_modules_of_types(pipe.unet,MonkeyIPAttnProcessor)
            print("kv",len(monkey_attn_list[0][1].kv))
            print("kv ip",len(monkey_attn_list[0][1].kv_ip))'''

            segmented=sam(gen_image,dim,dim)

            left=concat_images_vertically([ip_adapter_image,gen_image,segmented])

            from PIL import Image, ImageOps

            text_inputs = pipe.tokenizer(
                            prompt,
                            padding="max_length",
                            max_length=pipe.tokenizer.model_max_length,
                            truncation=True,
                            return_tensors="pt",
                        )
            text_input_ids = text_inputs.input_ids[0]

            for layer_index in [15]:
                [name,module]=attn_list[layer_index]
                if getattr(module,"processor",None)!=None and type(getattr(module,"processor",None))==MonkeyIPAttnProcessor:
                    processor_kv=module.processor.kv
                    vertical_image_list=[]
                    for token in range(n_tokens):
                        token_id=text_input_ids[token]
                        decoded=pipe.tokenizer.decode(token_id)
                        horiz_image_list=[]
                        for step in range(num_inference_steps):
                            size=processor_kv[step+count].size()
                            latent_dim=int(math.sqrt(size[2]))
                            avg=processor_kv[step].mean(dim=1).squeeze(0)
                            avg=avg.view([latent_dim,latent_dim,-1])
                            avg=avg[:,:,token]
                            avg_min,avg_max=avg.min(),avg.max()
                            x_norm = (avg - avg_min) / (avg_max - avg_min)  # [0,1]
                            avg = (x_norm * 255).byte()
                            avg=F.interpolate(avg.unsqueeze(0).unsqueeze(0), size=(dim, dim), mode="nearest").squeeze(0).squeeze(0)
                            bw_img = Image.fromarray(avg.cpu().numpy(), mode="L")  # "L" = 8-bit grayscale
                            mask = ImageOps.invert(bw_img)
                            color_rgba = gen_image.convert("RGB")
                            mask = mask.convert("RGB")  # must be single channel for alpha

                            #print(mask.size,color_rgba.size)

                            # Apply as alpha (translucent mask)
                            new_img=Image.blend(color_rgba, mask, 0.5)
                            horiz_image_list.append(new_img)
                        horiz_image=concat_images_horizontally(horiz_image_list)
                        horiz_image=add_padding_with_text(horiz_image, decoded,pad_width=dim,font_size=dim//4)
                        vertical_image_list.append(horiz_image)
                    processor_kv=module.processor.kv_ip
                    for token in range(n_tokens_ip):
                        token_id=text_input_ids[token]
                        decoded=f"ip_{token}"
                        horiz_image_list=[]
                        for step in range(num_inference_steps):
                            size=processor_kv[step+count_ip].size()
                            latent_dim=int(math.sqrt(size[2]))
                            avg=processor_kv[step].mean(dim=1).squeeze(0)
                            avg=avg.view([latent_dim,latent_dim,-1])
                            avg=avg[:,:,token]
                            avg_min,avg_max=avg.min(),avg.max()
                            x_norm = (avg - avg_min) / (avg_max - avg_min)  # [0,1]
                            x_norm[x_norm < threshold]=0.
                            avg = (x_norm * 255).byte()
                            avg=F.interpolate(avg.unsqueeze(0).unsqueeze(0), size=(dim, dim), mode="nearest").squeeze(0).squeeze(0)
                            bw_img = Image.fromarray(avg.cpu().numpy(), mode="L")  # "L" = 8-bit grayscale
                            mask = ImageOps.invert(bw_img)
                            color_rgba = gen_image.convert("RGB")
                            mask = mask.convert("RGB")  # must be single channel for alpha

                            # Apply as alpha (translucent mask)
                            new_img=Image.blend(color_rgba, mask, 0.5)
                            horiz_image_list.append(new_img)
                        horiz_image=concat_images_horizontally(horiz_image_list)
                        horiz_image=add_padding_with_text(horiz_image, decoded,pad_width=dim,font_size=dim//4)
                        vertical_image_list.append(horiz_image)
                    vertical_image=concat_images_vertically(vertical_image_list)
                    vertical_height=vertical_image.size[0]
                    left_height=left.size[0]
                    new_left=add_margin(left,0,0,vertical_height-left_height,0,"white")
                    vertical_image=concat_images_horizontally([new_left,vertical_image])
                    vertical_image.save(f"ip_images/{m}_{n}_layer_{layer_index}.png")
            '''count+=n_tokens
            count+=n_tokens_ip'''
    print("all done!")