# -*- coding: utf-8 -*-
"""ipattn.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1m30_6pz-5y_4u_2g2qvR7PCONL7SZIZ8
"""

import torch
from diffusers import StableDiffusionPipeline, AutoencoderKL
from transformers import CLIPImageProcessor, CLIPVisionModelWithProjection
import matplotlib.pyplot as plt
from embedding_helpers import EmbeddingUtil
from pipelines import CompatibleLatentConsistencyModelPipeline
from PIL import Image
import gc
from controlnet_aux import HEDdetector, MidasDetector, MLSDdetector, OpenposeDetector, PidiNetDetector, NormalBaeDetector, LineartDetector, LineartAnimeDetector, CannyDetector, ContentShuffleDetector, ZoeDetector, MediapipeFaceDetector, SamDetector, LeresDetector, DWposeDetector
from custom_sam_detector import CustomSamDetector

import os
import warnings
from typing import Union

import cv2
import numpy as np
import torch
from huggingface_hub import hf_hub_download
from PIL import Image
from diffusers.loaders.ip_adapter import IPAdapterMixin

# Load human segmentation preprocessor
sam =  SamDetector.from_pretrained("ybelkada/segment-anything", subfolder="checkpoints")

from diffusers.utils.loading_utils import load_image
from huggingface_hub import create_repo,HfApi


from diffusers.models.attention_processor import  IPAdapterAttnProcessor, IPAdapterAttnProcessor2_0, IPAdapterXFormersAttnProcessor,Attention
from diffusers.utils import deprecate, is_torch_xla_available, logging
from typing import Optional,List
from diffusers.image_processor import IPAdapterMaskProcessor
# Copyright 2025 The HuggingFace Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from adapter_helpers import replace_ip_attn,get_modules_of_types
import math

import torch
import torch.nn.functional as F
from image_utils import concat_images_horizontally,concat_images_vertically
from PIL import Image, ImageDraw, ImageFont
from diffusers.loaders.unet_loader_utils import _maybe_expand_lora_scales
import os
import sys
from ipattn import MonkeyIPAttnProcessor, reset_monkey, add_margin, add_padding_with_text

if __name__ =="__main__":
    folder="ip_images"
    
    use_embedding=False
    clargs=sys.argv
    print("clargs",clargs)
    if len(clargs) > 1:
        embedding_type=clargs[1]
        if embedding_type in ["clip","ssl","dino","siglip2"]:
            use_embedding=True


    count=0
    count_ip=0
    ip_adapter_image=load_image("https://assets-us-01.kc-usercontent.com/5cb25086-82d2-4c89-94f0-8450813a0fd3/0c3fcefb-bc28-4af6-985e-0c3b499ae832/Elon_Musk_Royal_Society.jpg")

    pipe = StableDiffusionPipeline.from_pretrained(
        "SimianLuo/LCM_Dreamshaper_v7",
        torch_dtype=torch.float16,
    ).to("cuda")

    # Load IP-Adapter
    pipe.load_ip_adapter("h94/IP-Adapter", subfolder="models", weight_name="ip-adapter_sd15.bin")

    if use_embedding:
        pipe = CompatibleLatentConsistencyModelPipeline.from_pretrained(
            "SimianLuo/LCM_Dreamshaper_v7",
            torch_dtype=torch.float16,
        ).to("cuda")

        # Load IP-Adapter
        pipe.load_ip_adapter("h94/IP-Adapter", subfolder="models", weight_name="ip-adapter_sd15.bin")
        facet="query"
        dino_pooling_stride=4
        embedding_util=EmbeddingUtil(pipe.unet.device,torch.float16,embedding_type,facet,dino_pooling_stride)
        lr=0.001
        suffix="_identity"
        n =1000
        pipeline_name="lcm"
        reward_switch_epoch =-1
        hf_path=f"jlbaker361/denoise_epsilon_{embedding_type}_1.0_{lr}_{n}{suffix}_{pipeline_name}_{reward_switch_epoch}"

        embedding_dim={
            "clip":768,
            "siglip2":768,
            "ssl":1536,
            "dino":3456
        }[embedding_type]

        num_image_text_embeds=4
        intermediate_embedding_dim=1024

        cross_attention_dim=embedding_dim//num_image_text_embeds
        use_projection=True
        identity_adapter=True
        deep_to_ip_layers=False
        WEIGHTS_NAME="unet_model.bin"
        api=HfApi()
        replace_ip_attn(pipe.unet,
                    embedding_dim,
                    intermediate_embedding_dim,
                    cross_attention_dim,
                    num_image_text_embeds,
                    use_projection,identity_adapter,deep_to_ip_layers)
        pretrained_weights_path=api.hf_hub_download(hf_path,WEIGHTS_NAME,force_download=True)
        pipe.unet.load_state_dict(torch.load(pretrained_weights_path,weights_only=True),strict=False)
        
        print("loaded from  ",pretrained_weights_path)
        folder=folder+"_"+embedding_type


    pipe.set_ip_adapter_scale(0.5)
    os.makedirs(folder,exist_ok=True)

    

    attn_list=get_modules_of_types(pipe.unet,Attention)
    for [name,_] in attn_list:
        print(name)

    for name,module in attn_list:
        if getattr(module,"processor",None)!=None and type(getattr(module,"processor",None))==IPAdapterAttnProcessor2_0:
            setattr(module,"processor",MonkeyIPAttnProcessor(module.processor,name))
    dim=512

    setattr(pipe,"safety_checker",None)

    n_tokens=6
    n_tokens_ip=4

    threshold=0.5

    gen=torch.Generator()
    gen.manual_seed(123)
    num_inference_steps=4
    for n,ip_adapter_image in enumerate([
        load_image("https://assets-us-01.kc-usercontent.com/5cb25086-82d2-4c89-94f0-8450813a0fd3/0c3fcefb-bc28-4af6-985e-0c3b499ae832/Elon_Musk_Royal_Society.jpg"),
        load_image("https://ddragon.leagueoflegends.com/cdn/img/champion/splash/Annie_1.jpg"),
        load_image("https://ddragon.leagueoflegends.com/cdn/img/champion/splash/Braum_2.jpg"),
        load_image("https://ddragon.leagueoflegends.com/cdn/img/champion/splash/Ezreal_2.jpg"),
        load_image("https://draftsim.com/wp-content/uploads/2020/06/Oath-of-Teferi-MTG-card-art-by-Wesley-Burt-1024x752.jpg"),
        load_image("https://ddragon.leagueoflegends.com/cdn/img/champion/splash/Hwei_1.jpg"),
        load_image("https://ddragon.leagueoflegends.com/cdn/img/champion/splash/Leblanc_1.jpg"),
        load_image("elf.jpg"),
        load_image("ghibli.jpg")
    ]):
        for m,prompt in enumerate(["eating ice cream","in paris","in the style of cubism","on a walk"]):
            #reset_monkey(pipe)

            '''pipe = StableDiffusionPipeline.from_pretrained(
                "SimianLuo/LCM_Dreamshaper_v7",
                torch_dtype=torch.float16,
            ).to("cuda")

            # Load IP-Adapter
            pipe.load_ip_adapter("h94/IP-Adapter", subfolder="models", weight_name="ip-adapter_sd15.bin")
            pipe.set_ip_adapter_scale(0.5)

            

            attn_list=get_modules_of_types(pipe.unet,Attention)

            for name,module in attn_list:
                if getattr(module,"processor",None)!=None and type(getattr(module,"processor",None))==IPAdapterAttnProcessor2_0:
                    setattr(module,"processor",MonkeyIPAttnProcessor(module.processor,name))
            dim=512

            setattr(pipe,"safety_checker",None)'''

            reset_monkey(pipe)
            if use_embedding:
                image_embeds=embedding_util.embed_img_tensor(pipe.image_processor.preprocess(image=ip_adapter_image)).unsqueeze(0).unsqueeze(0)
                gen_image=pipe(prompt,height=dim,width=dim,num_inference_steps=num_inference_steps,ip_adapter_image_embeds=[image_embeds],generator=gen).images[0]
            else:
                gen_image=pipe(prompt,height=dim,width=dim,num_inference_steps=num_inference_steps,ip_adapter_image=ip_adapter_image,generator=gen).images[0]
            
            '''monkey_attn_list=get_modules_of_types(pipe.unet,MonkeyIPAttnProcessor)
            print("kv",len(monkey_attn_list[0][1].kv))
            print("kv ip",len(monkey_attn_list[0][1].kv_ip))'''

            segmented=sam(gen_image,dim,dim)

            left=concat_images_vertically([ip_adapter_image,gen_image,segmented])

            from PIL import Image, ImageOps

            text_inputs = pipe.tokenizer(
                            prompt,
                            padding="max_length",
                            max_length=pipe.tokenizer.model_max_length,
                            truncation=True,
                            return_tensors="pt",
                        )
            text_input_ids = text_inputs.input_ids[0]

            for layer_index in [15]:
                [name,module]=attn_list[layer_index]
                if getattr(module,"processor",None)!=None and type(getattr(module,"processor",None))==MonkeyIPAttnProcessor:
                    processor_kv=module.processor.kv
                    vertical_image_list=[]
                    for token in range(n_tokens):
                        token_id=text_input_ids[token]
                        decoded=pipe.tokenizer.decode(token_id)
                        horiz_image_list=[]
                        for step in range(num_inference_steps):
                            size=processor_kv[step+count].size()
                            latent_dim=int(math.sqrt(size[2]))
                            avg=processor_kv[step].mean(dim=1).squeeze(0)
                            avg=avg.view([latent_dim,latent_dim,-1])
                            avg=avg[:,:,token]
                            avg_min,avg_max=avg.min(),avg.max()
                            x_norm = (avg - avg_min) / (avg_max - avg_min)  # [0,1]
                            avg = (x_norm * 255).byte()
                            avg=F.interpolate(avg.unsqueeze(0).unsqueeze(0), size=(dim, dim), mode="nearest").squeeze(0).squeeze(0)
                            bw_img = Image.fromarray(avg.cpu().numpy(), mode="L")  # "L" = 8-bit grayscale
                            mask = ImageOps.invert(bw_img)
                            color_rgba = gen_image.convert("RGB")
                            mask = mask.convert("RGB")  # must be single channel for alpha

                            #print(mask.size,color_rgba.size)

                            # Apply as alpha (translucent mask)
                            new_img=Image.blend(color_rgba, mask, 0.5)
                            horiz_image_list.append(new_img)
                        horiz_image=concat_images_horizontally(horiz_image_list)
                        horiz_image=add_padding_with_text(horiz_image, decoded,pad_width=dim,font_size=dim//4)
                        vertical_image_list.append(horiz_image)
                    processor_kv=module.processor.kv_ip
                    for token in range(n_tokens_ip):
                        token_id=text_input_ids[token]
                        decoded=f"ip_{token}"
                        horiz_image_list=[]
                        for step in range(num_inference_steps):
                            size=processor_kv[step+count_ip].size()
                            latent_dim=int(math.sqrt(size[2]))
                            avg=processor_kv[step].mean(dim=1).squeeze(0)
                            avg=avg.view([latent_dim,latent_dim,-1])
                            avg=avg[:,:,token]
                            avg_min,avg_max=avg.min(),avg.max()
                            x_norm = (avg - avg_min) / (avg_max - avg_min)  # [0,1]
                            x_norm[x_norm < threshold]=0.
                            avg = (x_norm * 255).byte()
                            avg=F.interpolate(avg.unsqueeze(0).unsqueeze(0), size=(dim, dim), mode="nearest").squeeze(0).squeeze(0)
                            bw_img = Image.fromarray(avg.cpu().numpy(), mode="L")  # "L" = 8-bit grayscale
                            mask = ImageOps.invert(bw_img)
                            color_rgba = gen_image.convert("RGB")
                            mask = mask.convert("RGB")  # must be single channel for alpha

                            # Apply as alpha (translucent mask)
                            new_img=Image.blend(color_rgba, mask, 0.5)
                            horiz_image_list.append(new_img)
                        horiz_image=concat_images_horizontally(horiz_image_list)
                        horiz_image=add_padding_with_text(horiz_image, decoded,pad_width=dim,font_size=dim//4)
                        vertical_image_list.append(horiz_image)
                    vertical_image=concat_images_vertically(vertical_image_list)
                    vertical_height=vertical_image.size[0]
                    left_height=left.size[0]
                    new_left=add_margin(left,0,0,vertical_height-left_height,0,"white")
                    vertical_image=concat_images_horizontally([new_left,vertical_image])
                    vertical_image.save(f"{folder}/{m}_{n}_layer_{layer_index}.png")
            '''count+=n_tokens
            count+=n_tokens_ip'''
    print("all done!")