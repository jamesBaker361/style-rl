# -*- coding: utf-8 -*-
"""ipattn.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1m30_6pz-5y_4u_2g2qvR7PCONL7SZIZ8
"""

import torch
from diffusers import StableDiffusionPipeline, AutoencoderKL
from transformers import CLIPImageProcessor, CLIPVisionModelWithProjection
import matplotlib.pyplot as plt
from embedding_helpers import EmbeddingUtil
from pipelines import CompatibleLatentConsistencyModelPipeline
from PIL import Image
import gc
from controlnet_aux import HEDdetector, MidasDetector, MLSDdetector, OpenposeDetector, PidiNetDetector, NormalBaeDetector, LineartDetector, LineartAnimeDetector, CannyDetector, ContentShuffleDetector, ZoeDetector, MediapipeFaceDetector, SamDetector, LeresDetector, DWposeDetector
from custom_sam_detector import CustomSamDetector

import os
import warnings
from typing import Union

import cv2
import numpy as np
import torch
from huggingface_hub import hf_hub_download
from PIL import Image
from diffusers.loaders.ip_adapter import IPAdapterMixin

# Load human segmentation preprocessor
sam =  SamDetector.from_pretrained("ybelkada/segment-anything", subfolder="checkpoints")

from diffusers.utils.loading_utils import load_image
from huggingface_hub import create_repo,HfApi


from diffusers.models.attention_processor import  IPAdapterAttnProcessor, IPAdapterAttnProcessor2_0, IPAdapterXFormersAttnProcessor,Attention
from diffusers.utils import deprecate, is_torch_xla_available, logging
from typing import Optional,List
from diffusers.image_processor import IPAdapterMaskProcessor
# Copyright 2025 The HuggingFace Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from adapter_helpers import replace_ip_attn,get_modules_of_types
import math

import torch
import torch.nn.functional as F
from image_utils import concat_images_horizontally,concat_images_vertically
from PIL import Image, ImageDraw, ImageFont
from diffusers.loaders.unet_loader_utils import _maybe_expand_lora_scales
import os
import sys
from ipattn import MonkeyIPAttnProcessor, reset_monkey, add_margin, add_padding_with_text
from PIL import Image, ImageDraw, ImageFont
import textwrap
from typing import Optional

def add_caption_below_image_obj(
    image: Image.Image,
    text: str,
    font_path: Optional[str] = None,
    font_size: int = 20,
    padding: int = 10,
    line_spacing: int = 4,
    rectangle_color=(255, 255, 255),
    text_color=(0, 0, 0),
    max_width_fraction: float = 0.95,
)->Image.Image:
    """
    Takes a PIL Image object, appends a white rectangle with text,
    and saves to output_path.
    """
    img = image
    img_w, img_h = img.size

    # Choose font
    if font_path:
        font = ImageFont.truetype(font_path, font_size)
    else:
        font = ImageFont.load_default()

    # Create temporary draw to measure text
    tmp = Image.new("RGB", (img_w, 2000), (255, 255, 255))
    draw_tmp = ImageDraw.Draw(tmp)

    # Estimate max chars per line using average character width
    try:
        avg_char_w = font.getbbox("ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz")[2] / 52
    except Exception:
        avg_char_w = font.getsize("A")[0]

    max_text_width = int(img_w * max_width_fraction) - 2 * padding
    if max_text_width <= 0:
        max_chars = 40
    else:
        max_chars = max(20, int(max_text_width / max(avg_char_w, 1)))

    wrapped_lines = textwrap.wrap(text, width=max_chars)
    wrapped_text = "\n".join(wrapped_lines) if wrapped_lines else ""

    bbox = draw_tmp.multiline_textbbox((0, 0), wrapped_text, font=font, spacing=line_spacing)
    text_w = bbox[2] - bbox[0]
    text_h = bbox[3] - bbox[1]
    rect_h = text_h + 2 * padding

    # Handle mode
    if img.mode in ("RGBA", "LA") or ("transparency" in img.info):
        new_mode = "RGBA"
        background = (255, 255, 255, 0)
    else:
        new_mode = "RGB"
        background = (255, 255, 255)

    new_img = Image.new(new_mode, (img_w, img_h + rect_h), background)

    if new_mode == "RGBA":
        new_img.paste(img, (0, 0), img.convert("RGBA"))
    else:
        new_img.paste(img, (0, 0))

    draw = ImageDraw.Draw(new_img)

    rect_top = img_h
    rect_bottom = img_h + rect_h
    draw.rectangle([0, rect_top, img_w, rect_bottom], fill=rectangle_color)

    text_x = (img_w - text_w) // 2
    text_y = rect_top + padding

    if text_w + 2 * padding > img_w:
        max_chars = max(10, int(max_chars * 0.8))
        wrapped_lines = textwrap.wrap(text, width=max_chars)
        wrapped_text = "\n".join(wrapped_lines)
        bbox = draw.multiline_textbbox((0, 0), wrapped_text, font=font, spacing=line_spacing,font_size=font_size)
        text_w = bbox[2] - bbox[0]
        text_h = bbox[3] - bbox[1]
        text_x = (img_w - text_w) // 2

    draw.multiline_text((text_x, text_y), wrapped_text, font=font,
                        fill=text_color, spacing=line_spacing, align="center",font_size=font_size)
    
    return new_img

if __name__ =="__main__":
    folder="ip_images_multi"
    
    use_embedding=False
    clargs=sys.argv
    print("clargs",clargs)
    if len(clargs) > 1:
        embedding_type=clargs[1]
        if embedding_type in ["clip","ssl","dino","siglip2"]:
            use_embedding=True


    ip_adapter_image=load_image("https://assets-us-01.kc-usercontent.com/5cb25086-82d2-4c89-94f0-8450813a0fd3/0c3fcefb-bc28-4af6-985e-0c3b499ae832/Elon_Musk_Royal_Society.jpg")

    pipe = StableDiffusionPipeline.from_pretrained(
        "SimianLuo/LCM_Dreamshaper_v7",
        torch_dtype=torch.float16,
    ).to("cuda")

    # Load IP-Adapter
    pipe.load_ip_adapter("h94/IP-Adapter", subfolder="models", weight_name="ip-adapter_sd15.bin")

    if use_embedding:
        pipe = CompatibleLatentConsistencyModelPipeline.from_pretrained(
            "SimianLuo/LCM_Dreamshaper_v7",
            torch_dtype=torch.float16,
        ).to("cuda")

        # Load IP-Adapter
        pipe.load_ip_adapter("h94/IP-Adapter", subfolder="models", weight_name="ip-adapter_sd15.bin")
        facet="query"
        dino_pooling_stride=4
        embedding_util=EmbeddingUtil(pipe.unet.device,torch.float16,embedding_type,facet,dino_pooling_stride)
        lr=0.001
        suffix="_identity"
        n =1000
        pipeline_name="lcm"
        reward_switch_epoch =-1
        hf_path=f"jlbaker361/denoise_epsilon_{embedding_type}_1.0_{lr}_{n}{suffix}_{pipeline_name}_{reward_switch_epoch}"

        embedding_dim={
            "clip":768,
            "siglip2":768,
            "ssl":1536,
            "dino":3456
        }[embedding_type]

        num_image_text_embeds=4
        intermediate_embedding_dim=1024

        cross_attention_dim=embedding_dim//num_image_text_embeds
        use_projection=True
        identity_adapter=True
        deep_to_ip_layers=False
        WEIGHTS_NAME="unet_model.bin"
        api=HfApi()
        replace_ip_attn(pipe.unet,
                    embedding_dim,
                    intermediate_embedding_dim,
                    cross_attention_dim,
                    num_image_text_embeds,
                    use_projection,identity_adapter,deep_to_ip_layers)
        pretrained_weights_path=api.hf_hub_download(hf_path,WEIGHTS_NAME,force_download=True)
        pipe.unet.load_state_dict(torch.load(pretrained_weights_path,weights_only=True),strict=False)
        
        print("loaded from  ",pretrained_weights_path)
        folder=folder+"_"+embedding_type


    pipe.set_ip_adapter_scale(0.5)
    os.makedirs(folder,exist_ok=True)

    

    attn_list=get_modules_of_types(pipe.unet,Attention)
    for [name,_] in attn_list:
        print(name)

    for name,module in attn_list:
        if getattr(module,"processor",None)!=None and type(getattr(module,"processor",None))==IPAdapterAttnProcessor2_0:
            setattr(module,"processor",MonkeyIPAttnProcessor(module.processor,name))
    dim=512

    setattr(pipe,"safety_checker",None)

    n_tokens=6
    n_tokens_ip=4

    threshold=0.5

    gen=torch.Generator()
    gen.manual_seed(123)
    num_inference_steps=4
    image_list=[
       # load_image("https://assets-us-01.kc-usercontent.com/5cb25086-82d2-4c89-94f0-8450813a0fd3/0c3fcefb-bc28-4af6-985e-0c3b499ae832/Elon_Musk_Royal_Society.jpg"),
       # load_image("https://ddragon.leagueoflegends.com/cdn/img/champion/splash/Annie_1.jpg"),
       # load_image("https://ddragon.leagueoflegends.com/cdn/img/champion/splash/Braum_2.jpg"),
       # load_image("https://ddragon.leagueoflegends.com/cdn/img/champion/splash/Ezreal_2.jpg"),
        load_image("https://draftsim.com/wp-content/uploads/2020/06/Oath-of-Teferi-MTG-card-art-by-Wesley-Burt-1024x752.jpg"),
       # load_image("https://ddragon.leagueoflegends.com/cdn/img/champion/splash/Hwei_1.jpg"),
        load_image("https://ddragon.leagueoflegends.com/cdn/img/champion/splash/Leblanc_1.jpg"),
        load_image("elf.jpg"),
        load_image("ghibli.jpg")
    ]
    for n,ip_adapter_image in enumerate(image_list[1:]):
        for m,prompt in enumerate(["eating ice cream","in paris","in the style of cubism","on a walk"]):
            #reset_monkey(pipe)

            '''pipe = StableDiffusionPipeline.from_pretrained(
                "SimianLuo/LCM_Dreamshaper_v7",
                torch_dtype=torch.float16,
            ).to("cuda")

            # Load IP-Adapter
            pipe.load_ip_adapter("h94/IP-Adapter", subfolder="models", weight_name="ip-adapter_sd15.bin")
            pipe.set_ip_adapter_scale(0.5)

            

            attn_list=get_modules_of_types(pipe.unet,Attention)

            for name,module in attn_list:
                if getattr(module,"processor",None)!=None and type(getattr(module,"processor",None))==IPAdapterAttnProcessor2_0:
                    setattr(module,"processor",MonkeyIPAttnProcessor(module.processor,name))
            dim=512

            setattr(pipe,"safety_checker",None)'''

            first_image=image_list[n-1]

            reset_monkey(pipe)
            if use_embedding:
                image_embeds=embedding_util.embed_img_tensor(pipe.image_processor.preprocess(image=ip_adapter_image)).unsqueeze(0).unsqueeze(0)
                gen_image=pipe(prompt,height=dim,width=dim,num_inference_steps=num_inference_steps,ip_adapter_image_embeds=[image_embeds],generator=gen).images[0]
            else:
                gen_image=pipe(prompt,height=dim,width=dim,num_inference_steps=num_inference_steps,ip_adapter_image=[[first_image,ip_adapter_image]],generator=gen).images[0]
            
            '''monkey_attn_list=get_modules_of_types(pipe.unet,MonkeyIPAttnProcessor)
            print("kv",len(monkey_attn_list[0][1].kv))
            print("kv ip",len(monkey_attn_list[0][1].kv_ip))'''

            segmented=sam(gen_image,dim,dim)

            left=concat_images_vertically([ip_adapter_image,first_image, gen_image
                                           #segmented
                                           ])

            from PIL import Image, ImageOps

            text_inputs = pipe.tokenizer(
                            prompt,
                            padding="max_length",
                            max_length=pipe.tokenizer.model_max_length,
                            truncation=True,
                            return_tensors="pt",
                        )
            text_input_ids = text_inputs.input_ids[0]

            image_count=2

            for layer_index in [15]:
                [name,module]=attn_list[layer_index]
                if getattr(module,"processor",None)!=None and type(getattr(module,"processor",None))==MonkeyIPAttnProcessor:
                    processor_kv=module.processor.kv
                    vertical_image_list=[]
                    for token in range(n_tokens):
                        token_id=text_input_ids[token]
                        decoded=pipe.tokenizer.decode(token_id).replace("<|startoftext|>","<start>").replace("<|endoftext|>","<end>")
                        horiz_image_list=[]
                        for step in range(num_inference_steps):
                            
                            size=processor_kv[step].size()
                            latent_dim=int(math.sqrt(size[2]))
                            avg=processor_kv[step].mean(dim=1).squeeze(0)
                            avg=avg.view([latent_dim,latent_dim,-1])
                            avg=avg[:,:,token]
                            avg_min,avg_max=avg.min(),avg.max()
                            x_norm = (avg - avg_min) / (avg_max - avg_min)  # [0,1]
                            avg = (x_norm * 255).byte()
                            avg=F.interpolate(avg.unsqueeze(0).unsqueeze(0), size=(dim, dim), mode="nearest").squeeze(0).squeeze(0)
                            bw_img = Image.fromarray(avg.cpu().numpy(), mode="L")  # "L" = 8-bit grayscale
                            mask = ImageOps.invert(bw_img)
                            color_rgba = gen_image.convert("RGB")
                            mask = mask.convert("RGB")  # must be single channel for alpha

                            #print(mask.size,color_rgba.size)

                            # Apply as alpha (translucent mask)
                            new_img=Image.blend(color_rgba, mask, 0.5)
                            horiz_image_list.append(new_img)
                        horiz_image=concat_images_horizontally(horiz_image_list)
                        horiz_image=add_padding_with_text(horiz_image, decoded,pad_width=dim,font_size=dim//4)
                        vertical_image_list.append(horiz_image)
                    processor_kv=module.processor.kv_ip
                    for token in range(n_tokens_ip):
                        token_id=text_input_ids[token]
                        
                        
                        
                        for i in range(image_count):
                            horiz_image_list=[]
                            decoded=f"ip_{token}_{i}"
                            for step in range(num_inference_steps):
                                index=step+image_count
                                size=processor_kv[index].size()
                                latent_dim=int(math.sqrt(size[2]))
                                avg=processor_kv[index].mean(dim=1).squeeze(0)
                                avg=avg.view([latent_dim,latent_dim,-1])
                                avg=avg[:,:,token]
                                avg_min,avg_max=avg.min(),avg.max()
                                x_norm = (avg - avg_min) / (avg_max - avg_min)  # [0,1]
                                x_norm[x_norm < threshold]=0.
                                avg = (x_norm * 255).byte()
                                avg=F.interpolate(avg.unsqueeze(0).unsqueeze(0), size=(dim, dim), mode="nearest").squeeze(0).squeeze(0)
                                bw_img = Image.fromarray(avg.cpu().numpy(), mode="L")  # "L" = 8-bit grayscale
                                mask = ImageOps.invert(bw_img)
                                color_rgba = gen_image.convert("RGB")
                                mask = mask.convert("RGB")  # must be single channel for alpha

                                # Apply as alpha (translucent mask)
                                new_img=Image.blend(color_rgba, mask, 0.5)
                                horiz_image_list.append(new_img)
                            horiz_image=concat_images_horizontally(horiz_image_list)
                            horiz_image=add_padding_with_text(horiz_image, decoded,pad_width=dim,font_size=dim//4)
                            vertical_image_list.append(horiz_image)
                    vertical_image=concat_images_vertically(vertical_image_list)
                    vertical_height=vertical_image.size[0]
                    left_height=left.size[0]
                    new_left=add_margin(left,0,0,(vertical_height-left_height)//2,0,"white")
                    new_left=add_margin(new_left,(vertical_height-left_height)//2,0,0,0,"white")
                    vertical_image=concat_images_horizontally([new_left,vertical_image])
                    vertical_image.save(f"{folder}/{m}_{n}_layer_{layer_index}.png")
            '''count+=n_tokens
            count+=n_tokens_ip'''
    print("all done!")