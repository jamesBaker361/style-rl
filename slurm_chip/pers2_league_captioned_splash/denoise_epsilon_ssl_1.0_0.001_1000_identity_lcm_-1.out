gcc (GCC) 13.3.0
Copyright (C) 2023 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.

Running on: g20-12
Allocated GPUs:
No devices were found
version
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2025 NVIDIA Corporation
Built on Wed_Jan_15_19:20:09_PST_2025
Cuda compilation tools, release 12.8, V12.8.61
Build cuda_12.8.r12.8/compiler.35404655_0
SLURMD_NODENAME g20-12
SBATCH_CLUSTERS doesnt exist
SBATCH_PARTITION doesnt exist
SLURM_JOB_PARTITION gpu
SLURM_NODEID 0
SLURM_MEM_PER_GPU doesnt exist
SLURM_MEM_PER_CPU doesnt exist
SLURM_MEM_PER_NODE 128000
SLURM_JOB_ID 98864
couldnt print cuda details
Namespace(dataset='jlbaker361/ssl-league_captioned_splash-1000', mixed_precision='fp16', project_name='league_captioned_splash-1000-beta', gradient_accumulation_steps=8, image_size=256, embedding='ssl', facet='query', pipeline='lcm', batch_size=2, epochs=1000, training_type='denoise', prediction_type='epsilon', train_split=0.95, validation_interval=40, uncaptioned_frac=1.0, intermediate_embedding_dim=1024, cross_attention_dim=768, limit=-1, num_inference_steps=20, dino_pooling_stride=4, num_image_text_embeds=4, deepspeed=False, fsdp=False, vanilla=True, name='jlbaker361/denoise_epsilon_ssl_1.0_0.001_1000_identity_lcm_-1', load=False, load_hf=True, upload_interval=1, generic_test_prompts=True, lr=0.001, disable_projection_adapter=False, identity_adapter=True, deep_to_ip_layers=False, scheduler_type='LCMScheduler', reward_switch_epoch=-1, initial_scale=1.0, final_scale=1.0, sigma_data=-0.8)
accelerator device cpu
Rank 0 initialized successfully
main process = 0

MODEL-NAME  denoise_epsilon_ssl_1.0_0.001_1000_identity_lcm_-1
LCMScheduler {
  "_class_name": "LCMScheduler",
  "_diffusers_version": "0.35.1",
  "beta_end": 0.012,
  "beta_schedule": "scaled_linear",
  "beta_start": 0.00085,
  "clip_sample": false,
  "clip_sample_range": 1.0,
  "dynamic_thresholding_ratio": 0.995,
  "num_train_timesteps": 1000,
  "original_inference_steps": 50,
  "prediction_type": "epsilon",
  "rescale_betas_zero_snr": false,
  "sample_max_value": 1.0,
  "set_alpha_to_one": true,
  "steps_offset": 1,
  "thresholding": false,
  "timestep_scaling": 10.0,
  "timestep_spacing": "leading",
  "trained_betas": null
}

scheduler add_noise exists
scheduler get_velocity exists
scheduler step exists
LCMScheduler {
  "_class_name": "LCMScheduler",
  "_diffusers_version": "0.35.1",
  "beta_end": 0.012,
  "beta_schedule": "scaled_linear",
  "beta_start": 0.00085,
  "clip_sample": false,
  "clip_sample_range": 1.0,
  "dynamic_thresholding_ratio": 0.995,
  "num_train_timesteps": 1000,
  "original_inference_steps": 50,
  "prediction_type": "epsilon",
  "rescale_betas_zero_snr": false,
  "sample_max_value": 1.0,
  "set_alpha_to_one": true,
  "steps_offset": 1,
  "thresholding": false,
  "timestep_scaling": 10.0,
  "timestep_spacing": "leading",
  "trained_betas": null
}

text size torch.Size([77, 768]) embedding size torch.Size([1, 1536]) img size torch.Size([3, 256, 256]) latent size torch.Size([8, 32, 32])
prompt list 1000
image_list 1000
text_list 1000
posterior list 1000
embedding list 1000
images are already denormalized max: 1.0 min: 7.569789886474609e-06, NOT denormalizing
use_projection and args.identity_adapter are both true
couldnt load from hf
Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.
len attn_layers 17
before  96
after 32
train/test/val (0.95, 0.025000000000000022, 0.025000000000000022)
prompt list 950
image_list 950
text_list 950
posterior list 950
embedding list 950
dataset batch <class 'dict'>
train batch <class 'dict'>
val batch <class 'dict'>
trainable params:  128
Rank 0 checkpoint
[1;34mwandb[0m: 
[1;34mwandb[0m: ðŸš€ View run [33mworldly-spaceship-985[0m at: [34mhttps://wandb.ai/jlbaker361/league_captioned_splash-1000-beta/runs/fw682sh3[0m
[1;34mwandb[0m: Find logs at: [1;35m../../../../../ada/donengel/common/wandb/wandb/run-20250919_074036-fw682sh3/logs[0m
Running on: g20-12
Allocated GPUs:
No devices were found
